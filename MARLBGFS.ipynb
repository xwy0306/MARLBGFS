{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_labels(data):\n",
    "    y = data['label'].values\n",
    "    X = data.drop(columns=['label']).values\n",
    "    return X, y\n",
    "def get_data(file_name):\n",
    "    file_path = os.path.join(config.DATA_DIR, file_name)\n",
    "\n",
    "\n",
    "    if 'train' in file_name:\n",
    "        data_train = pd.read_csv(file_path)  # 修正为使用 file_path\n",
    "        X_train, y_train = extract_features_labels(data_train)\n",
    "        test_file_path = file_path.replace(\"train\", \"test\")\n",
    "        data_test = pd.read_csv(test_file_path)\n",
    "        X_test, y_test = extract_features_labels(data_test)\n",
    "    else:\n",
    "        data = pd.read_csv(file_path)\n",
    "        label = data['label']\n",
    "        data = data.drop(columns=['label'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3, random_state=22)\n",
    "        # X_train, y_train, X_test, y_test = X_train.values, y_train.values, X_test.values, y_test.values\n",
    "\n",
    "    # sc = StandardScaler()\n",
    "    # X_train = sc.fit_transform(X_train)\n",
    "    # X_test = sc.transform(X_test)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    num_samples = X_train.shape[0] + X_test.shape[0]\n",
    "    num_features = X_train.shape[1]  \n",
    "    size = (num_samples, num_features)\n",
    "    print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)\n",
    "    return X_train, y_train, X_test, y_test, size\n",
    "\n",
    "def get_X_Y(file_path):\n",
    "    X_train, y_train, X_test, y_test, size = get_data(file_path)\n",
    "    if isinstance(X_train, pd.DataFrame) and isinstance(X_test, pd.DataFrame):\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "    if isinstance(y_train, pd.Series) and isinstance(y_test, pd.Series):\n",
    "        y_train = pd.Series(y_train)\n",
    "        y_test = pd.Series(y_test)  \n",
    "    X = pd.concat([X_train, X_test], axis=0)\n",
    "    y = pd.concat([y_train, y_test], axis=0)\n",
    "    return X, y, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37119742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import fcntl\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c62e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(file, accs, type):\n",
    "    file_path = \"/home/xiaowenyuan/Python/MGFS/picture/result/result.json\"\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if type in data:\n",
    "                data[type][file] = accs\n",
    "            else:\n",
    "                data[type] = {file: accs}\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"读取或写入文件 {file_path} 时出错: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                data = {type: {file: accs}}\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"创建并写入文件 {file_path} 时出错: {e}\")\n",
    "def save_result_csv(file_name, algorithm, classifier, size, dataSet, acc, f1, recall, selected_features,time=None):\n",
    "    # 打开文件并获取文件描述符\n",
    "    with open(file_name, 'a+') as file:\n",
    "        try:\n",
    "            # 尝试获取文件的排他锁，确保其他程序无法同时写入\n",
    "            fcntl.flock(file.fileno(), fcntl.LOCK_EX)\n",
    "            file.seek(0)  # 将文件指针移到文件开头\n",
    "            if file.read(1):  # 检查文件是否有内容\n",
    "                file.seek(0)  # 再次将文件指针移到文件开头\n",
    "                data = pd.read_csv(file)\n",
    "                new_result = {\n",
    "                    'algorithm': algorithm,\n",
    "                    'Classiflier': classifier,\n",
    "                    'rows': size[0],\n",
    "                    'columns': size[1],\n",
    "                    'dataSet': dataSet,\n",
    "                    'Accuracy': acc,  # 将标量转为列表\n",
    "                    'F1 Score': f1,\n",
    "                    'Recall': recall,\n",
    "                    'featureLength': len(selected_features),\n",
    "                    'features': selected_features\n",
    "                }\n",
    "                if time is not None:\n",
    "                    new_result['time']=time\n",
    "                results = pd.concat(\n",
    "                    [data, pd.DataFrame([new_result])],\n",
    "                    ignore_index=True\n",
    "                )\n",
    "            else:\n",
    "                results = pd.DataFrame({\n",
    "                    'algorithm': [algorithm],\n",
    "                    'Classiflier': [classifier],\n",
    "                    'rows': size[0],\n",
    "                    'columns': size[1],\n",
    "                    'dataSet': [dataSet],\n",
    "                    'Accuracy': [acc],  # 将标量转为列表\n",
    "                    'F1 Score': [f1],\n",
    "                    'Recall': [recall],\n",
    "                    'featureLength': [len(selected_features)],\n",
    "                    'features': [selected_features]\n",
    "                })\n",
    "                if time is not None:\n",
    "                    results['time']=[time]\n",
    "            # 保存结果到文件\n",
    "            results.to_csv(file_name, index=False)\n",
    "        finally:\n",
    "            # 释放文件锁\n",
    "            fcntl.flock(file.fileno(), fcntl.LOCK_UN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minepy import MINE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import timeit\n",
    "\n",
    "from net import Autoencoder, GAT, DQNAgent, Generator, Discriminator\n",
    "from util import PriorityReplayBuffer\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# clf = ExtraTreesClassifier(n_estimators=50, n_jobs=-1) #ExtraTreesClassifier(n_estimators=50, n_jobs=-1)极端随机树\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1)\n",
    "from sklearn.svm import SVC\n",
    "# clf = SVC(kernel='rbf', probability=True)\n",
    "from xgboost import XGBClassifier\n",
    "# clf = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, n_jobs=-1)\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from save_result import save_result_csv\n",
    "from read_csv import get_data\n",
    "import config\n",
    "\n",
    "device = config.DEVICE\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mi(X, n_features):\n",
    "    \"\"\"MI矩阵计算（特征数×特征数）\"\"\"\n",
    "    print(\"开始计算MI>>>>>>>\")\n",
    "    mi_matrix = np.zeros((n_features, n_features))\n",
    "    discrete_flags = [np.unique(X[:, i]).size < 10 for i in range(n_features)]  # 提前计算离散标志\n",
    "    # print(f\"离散特征标志计算完成:{discrete_flags}\")\n",
    "    for i in range(n_features):\n",
    "        # print(f\"正在计算第{i}个特征的MI\")\n",
    "        mi = mutual_info_regression(X, X[:, i], discrete_features=discrete_flags, n_jobs=-1)\n",
    "        mi_matrix[i, :] = mi\n",
    "\n",
    "    mi_matrix = (mi_matrix + mi_matrix.T) / 2  # 对称化\n",
    "    print(\"计算结束<<<<<<<<\")\n",
    "    return np.nan_to_num(mi_matrix)\n",
    "\n",
    "\n",
    "def feature_clustering_mi(mi_matrix, n_groups=15):\n",
    "    \"\"\"鲁棒的谱聚类\"\"\"\n",
    "    mi_matrix = np.maximum(mi_matrix, 0)\n",
    "    mi_matrix = (mi_matrix + mi_matrix.T) / 2\n",
    "    \n",
    "    try:\n",
    "        clustering = SpectralClustering(\n",
    "            n_clusters=n_groups,\n",
    "            affinity='precomputed',\n",
    "            random_state=42\n",
    "        ).fit(mi_matrix)\n",
    "    except Exception as e:\n",
    "        print(f\"谱聚类出错: {e}\")\n",
    "        n_features = mi_matrix.shape[0]\n",
    "        return [list(range(i, min(i + 22, n_features))) for i in range(0, n_features, 22)]\n",
    "    \n",
    "    feature_groups = [[] for _ in range(n_groups)]\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        feature_groups[label].append(idx)\n",
    "    return feature_groups\n",
    "\n",
    "\n",
    "def compute_mic(X, n_features):\n",
    "    \"\"\"MIC矩阵计算（特征数×特征数）\"\"\"\n",
    "    mic_matrix = np.zeros((n_features, n_features))\n",
    "    mine = MINE(alpha=0.6, c=15)\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        for j in range(i, n_features):\n",
    "            if np.std(X[:, i]) == 0 or np.std(X[:, j]) == 0:\n",
    "                mic = 0.0\n",
    "            else:\n",
    "                try:\n",
    "                    mine.compute_score(X[:, i], X[:, j])\n",
    "                    mic = mine.mic()\n",
    "                except Exception as e:\n",
    "                    print(f\"计算MIC出错: {e}\")\n",
    "                    mic = 0.0\n",
    "            mic_matrix[i, j] = mic_matrix[j, i] = mic\n",
    "\n",
    "    return np.nan_to_num(mic_matrix)\n",
    "\n",
    "\n",
    "def feature_clustering(mic_matrix, n_groups):\n",
    "    \"\"\"鲁棒的谱聚类\"\"\"\n",
    "    mic_matrix = np.maximum(mic_matrix, 0)\n",
    "    mic_matrix = (mic_matrix + mic_matrix.T) / 2\n",
    "    \n",
    "    try:\n",
    "        clustering = SpectralClustering(\n",
    "            n_clusters=n_groups,\n",
    "            affinity='precomputed',\n",
    "            random_state=42\n",
    "        ).fit(mic_matrix)\n",
    "    except Exception as e:\n",
    "        print(f\"谱聚类出错: {e}\")\n",
    "        n_features = mic_matrix.shape[0]\n",
    "        return [list(range(i, min(i + 22, n_features))) for i in range(0, n_features, 22)]\n",
    "    \n",
    "    feature_groups = [[] for _ in range(n_groups)]\n",
    "    for idx, label in enumerate(clustering.labels_):\n",
    "        feature_groups[label].append(idx)\n",
    "    return feature_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436766b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write_idx = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        idx = self.write_idx + self.capacity - 1\n",
    "        self.data[self.write_idx] = data \n",
    "        self.update(idx, priority)\n",
    "        self.write_idx = (self.write_idx + 1) % self.capacity\n",
    "\n",
    "    def update(self, idx, priority):\n",
    "        change = priority - self.tree[idx]\n",
    "        self.tree[idx] = priority\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        idx = 0\n",
    "        while True:\n",
    "            left = 2 * idx + 1\n",
    "            if left >= len(self.tree):\n",
    "                break\n",
    "            if v <= self.tree[left]:\n",
    "                idx = left\n",
    "            else:\n",
    "                v -= self.tree[left]\n",
    "                idx = left + 1\n",
    "        return self.data[idx - self.capacity + 1]\n",
    "\n",
    "\n",
    "class PriorityReplayBuffer:\n",
    "    def __init__(self, real_capacity, gen_capacity, alpha=1, beta=0.5):\n",
    "        self.real_tree = SumTree(real_capacity)\n",
    "        self.gen_tree = SumTree(gen_capacity)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "    def add_real(self, transition, priority):\n",
    "        self.real_tree.add(priority ** self.alpha, transition)\n",
    "\n",
    "    def add_gen(self, transition, priority):\n",
    "        self.gen_tree.add(priority ** self.alpha, transition)\n",
    "\n",
    "    def get_idx(self):\n",
    "        return self.real_tree.write_idx\n",
    "\n",
    "    def sample(self, batch_size, p=0.5):\n",
    "        real_batch_size = int(batch_size * p)\n",
    "        gen_batch_size = batch_size - real_batch_size\n",
    "\n",
    "        # 真实经验采样\n",
    "        real_total = self.real_tree.tree[0]\n",
    "        gen_total = self.gen_tree.tree[0]\n",
    "        total = real_total + gen_total + 1e-5\n",
    "\n",
    "        real_segment = real_total / total\n",
    "        real_batch = [self.real_tree.get_leaf(np.random.uniform(0, real_segment)) for _ in range(real_batch_size)]\n",
    "\n",
    "        # 生成经验采样\n",
    "        gen_batch = [self.gen_tree.get_leaf(np.random.uniform(real_segment, 1)) for _ in range(gen_batch_size)]\n",
    "\n",
    "        return real_batch + gen_batch\n",
    "\n",
    "    def sample_true(self, batch_size):\n",
    "        real_total = self.real_tree.tree[0]\n",
    "        return [self.real_tree.get_leaf(np.random.uniform(0, real_total)) for _ in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b74aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelectionEnv:\n",
    "    def __init__(self,  X, y, X_test, y_test):  \n",
    "        # 数据存储\n",
    "        self.X = torch.FloatTensor(X.T).to(device)  # [特征数, 样本数]，移动到GPU\n",
    "        self.y = y\n",
    "        self.X_test,self.y_test = X_test,y_test\n",
    "        self.cache = {}  # 用于存储特征选择结果的缓存\n",
    "        self.num_features, self.num_samples = self.X.shape\n",
    "        # self.feature_groups = feature_groups\n",
    "        \n",
    "        # self.mi_matrix = mi_matrix  # 存储MI矩阵\n",
    "        # self.pre_edge_index = self._build_graph(mi_matrix)\n",
    "        \n",
    "        # 初始化模型，并移动到GPU\n",
    "        self.ae = Autoencoder(self.num_features, self.num_samples).to(device)\n",
    "        # self.gat = GAT(64).to(device)\n",
    "        self.gat = GAT(64).to(device)\n",
    "        \n",
    "        # 预训练与验证\n",
    "        self._pretrain_ae()\n",
    "        self.z_ae, self.d_ae = self.ae(self.X)\n",
    "        print(f\"z_ae shape: {self.z_ae.shape}\")  # 打印z_ae的形状\n",
    "        self.z_ae = self.z_ae.detach()  # 分离梯度，避免更新\n",
    "        self.X_ae = self.z_ae.T.cpu()\n",
    "        self.mi_matrix = compute_mi(self.X_ae, X.shape[1])\n",
    "        laplacian = np.diag(np.sum(self.mi_matrix, axis=1)) - self.mi_matrix\n",
    "        eigvals = np.sort(np.linalg.eigvalsh(laplacian))\n",
    "        \n",
    "        # 寻找特征值拐点（经验法则）\n",
    "        eig_diff = np.diff(eigvals[:20])\n",
    "        n_groups = np.argmax(eig_diff) + 1  # 加1因为diff后索引偏移\n",
    "        # n_groups_num = X_train.shape[1] // 5 + 1\n",
    "        if n_groups < X_train.shape[1] // 10 + 1:\n",
    "            n_groups = X_train.shape[1] // 5 + 1\n",
    "        print(f\"特征分组数量: {n_groups}\")\n",
    "        self.feature_groups = feature_clustering_mi(self.mi_matrix, n_groups)\n",
    "        self.n_agents = len(self.feature_groups)\n",
    "        # print(f\"z_ae shape: {self.z_ae.shape}\")  # 打印z_ae的形状\n",
    "\n",
    "    def _build_graph(self, feature_select, top_k=5):\n",
    "        \"\"\"基于MI矩阵和已选特征的图结构构建\"\"\"\n",
    "        edge_indices = []\n",
    "        \n",
    "        if len(feature_select) == 0:\n",
    "            # 原有互信息拓扑连接\n",
    "            for i in range(self.num_features):\n",
    "                sorted_indices = np.argsort(self.mi_matrix[i])[::-1]\n",
    "                valid_neighbors = [\n",
    "                    j for j in sorted_indices \n",
    "                    if j != i and j < self.mi_matrix.shape[0]\n",
    "                ][:top_k]\n",
    "                edge_indices.extend([[i, j] for j in valid_neighbors])\n",
    "        else:\n",
    "            # 仅对已选特征进行构图\n",
    "            for i in feature_select:\n",
    "                sorted_indices = np.argsort(self.mi_matrix[i])[::-1]\n",
    "                # 筛选出同样在已选特征中的邻居\n",
    "                valid_neighbors = [\n",
    "                    j for j in sorted_indices \n",
    "                    if j != i and j in feature_select\n",
    "                ][:top_k]\n",
    "                edge_indices.extend([[i, j] for j in valid_neighbors])\n",
    "        \n",
    "        # print(f\"最终边数量: {len(edge_indices)} (含组内全连接)\")  # 修正打印边数量的错误\n",
    "        edge_tensor = torch.LongTensor(edge_indices).t().contiguous().to(device)\n",
    "        return edge_tensor\n",
    "    def _pretrain_ae(self, epochs=50):\n",
    "        \"\"\"带梯度裁剪的预训练\"\"\"\n",
    "        optimizer = optim.AdamW(self.ae.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()  # 提前定义损失函数\n",
    "        for _ in range(epochs): \n",
    "            _, recon = self.ae(self.X)\n",
    "            loss = criterion(recon, self.X)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.ae.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "    def get_state(self, feature_select):\n",
    "        with torch.no_grad():\n",
    "            edge_index = self._build_graph(feature_select)\n",
    "            gat_out = self.gat(self.z_ae, edge_index)  # [num_features, hidden_dim]\n",
    "            # print(gat_out.shape)\n",
    "        return gat_out.mean(dim=0)  # [hidden_dim]\n",
    "\n",
    "    def calculate_reward(self, selected_features, data_loader, clf):\n",
    "        if len(selected_features) == 0:\n",
    "            return -1.0, 0.0\n",
    "        selected_features = sorted(selected_features)\n",
    "        if str(selected_features) in self.cache:\n",
    "            reward, acc = self.cache[str(selected_features)]\n",
    "            return reward, acc\n",
    "        else:\n",
    "            acc = 0.0\n",
    "            try:\n",
    "                # 使用 enumerate 的第二个参数 start=0 明确起始索引，遍历不超过 MAX_BATCHES 个批次的数据\n",
    "                for i, (batch_X, batch_y) in enumerate(data_loader, start=0):\n",
    "                    batch_X_selected = batch_X[:, selected_features]\n",
    "                    \n",
    "                    # 训练分类器并计算准确度\n",
    "                    clf.fit(batch_X_selected, batch_y)\n",
    "                    # y_pred = clf.predict(self.X_test[:, selected_features])\n",
    "                    # acc = accuracy_score(self.y_test, y_pred)\n",
    "                    acc = clf.score(batch_X_selected, batch_y)\n",
    "            except Exception as e:  # 明确捕获异常类型\n",
    "                print(f\"z_ae 形状: {self.z_ae.shape}\")\n",
    "                print(f\"LogisticRegression 训练出错: {e}\")\n",
    "            redundancy = sum(\n",
    "                np.mean(self.mi_matrix[group][:, group]) \n",
    "                for group in self.feature_groups \n",
    "                if len(group) > 1\n",
    "            ) / self.num_features\n",
    "            # reward = 0.8 * acc + 0.2 * (0.5 * (1 - redundancy) + 0.5 * (1 - len(selected_features) / self.num_features))\n",
    "            redundancy_penalty = 1 - redundancy\n",
    "            length_penalty = 1 - len(selected_features) / self.num_features\n",
    "            reward = 0.9 * acc + 0.1 * redundancy_penalty \n",
    "            self.cache[str(selected_features)] = (reward, acc)\n",
    "            return reward, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414392d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentSystem:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # 初始化智能体并移动到GPU\n",
    "        self.agents = [(DQNAgent(env.gat.output_dim).to(device), DQNAgent(env.gat.output_dim, False).to(device)) for _ in range(env.n_agents)]\n",
    "        self.update_target = 10\n",
    "        self.optimizers = [optim.Adam(agent[0].parameters(), lr=1e-4) for agent in self.agents]\n",
    "        # 初始化生成器和判别器并移动到GPU\n",
    "        self.generators = [Generator(env.gat.output_dim, 1).to(device) for _ in range(env.n_agents)]\n",
    "        self.discriminators = [Discriminator(env.gat.output_dim, 1).to(device) for _ in range(env.n_agents)]\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 32\n",
    "        # self.buffer_tree = PriorityReplayBuffer(100)\n",
    "        self.train_GAN_batch_size = 64\n",
    "        self.TARGET_REPLACE_ITER = 50\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.data_loader = DataLoader(torch.utils.data.TensorDataset(\n",
    "                self.env.X.T.cpu(),  # 转换为 [样本数, 特征数] 并移动到 CPU\n",
    "                torch.tensor(self.env.y, dtype=torch.float32).cpu()  # 确保标签是张量并移动到 CPU\n",
    "            ), batch_size=256, shuffle=True)\n",
    "    def generate_and_add_fake_data(self, z_size):\n",
    "        for agent_idx in range(self.env.n_agents):\n",
    "            z = torch.randn(z_size, 32).to(device)  # 移动到GPU\n",
    "            fake_datas = self.generators[agent_idx](z)\n",
    "            split_sizes = [self.env.gat.output_dim, 1, 1, self.env.gat.output_dim]\n",
    "            s_gen_all, a_gen_all, r_gen_all, s_next_gen_all = fake_datas.split(split_sizes, dim=1)\n",
    "            a_gen_all = (a_gen_all > 0.5).int()\n",
    "            # with torch.no_grad():\n",
    "            #     # 计算生成数据的TD误差\n",
    "            #     q_values = self.agents[agent_idx][0](s_gen_all)\n",
    "            #     next_q_values = self.agents[agent_idx][1](s_next_gen_all)\n",
    "            #     td_errors = torch.abs(r_gen_all + self.gamma * next_q_values.max(dim=1)[0].unsqueeze(1) - q_values.gather(1, a_gen_all.long()))\n",
    "                \n",
    "                # 对每个样本计算优先级\n",
    "            priorities = (r_gen_all + 1e-5)\n",
    "            transitions = [self._to_transition(s_gen_all[i], r_gen_all[i], int(a_gen_all[i]), s_next_gen_all[i]) for i in range(z_size)]\n",
    "            for i in range(z_size):\n",
    "                transition = transitions[i]\n",
    "                self.agents[agent_idx][0].memory.add_gen(transition, priorities[i])\n",
    "    def train_with_gan(self, clf, dqn_episodes=500):\n",
    "        epsilon = 1.0\n",
    "        epsilon_decay = 0.98\n",
    "        min_epsilon = 0.05\n",
    "        state = self.env.get_state([]).to(device)  # 移动到GPU\n",
    "        Flag = True\n",
    "\n",
    "        group_features = {\n",
    "            i: set(group) \n",
    "            for i, group in enumerate(self.env.feature_groups)\n",
    "        }\n",
    "        results = {\"reward\":0, \"episode\":0, \"features\":[], \"acc\":0}\n",
    "        for episode in range(dqn_episodes):\n",
    "            actions = []\n",
    "            while 1 not in actions:\n",
    "                actions = [eval_agent.act(state, epsilon) for (eval_agent, target_agent) in self.agents]\n",
    "            selected_features = [f for group, action in zip(self.env.feature_groups, actions) if action == 1 for f in group]\n",
    "\n",
    "            full_features = set(selected_features)\n",
    "          \n",
    "            # 随机选取最多 10 个批次并合并\n",
    "            data_loader_list = list(self.data_loader)\n",
    "            selected_batches = random.sample(data_loader_list, min(10, len(data_loader_list)))\n",
    "            if selected_batches:\n",
    "                merged_X, merged_y = zip(*selected_batches)\n",
    "                merged_X = torch.cat(merged_X, dim=0)\n",
    "                merged_y = torch.cat(merged_y, dim=0).to(torch.int32)\n",
    "                selected_batches = [(merged_X, merged_y)]\n",
    "\n",
    "            # 直接使用选取的批次调用 calculate_reward\n",
    "            full_reward, acc = self.env.calculate_reward(list(full_features), selected_batches, clf)\n",
    "            next_state = self.env.get_state(selected_features).to(device)  # 移动到GPU\n",
    "            if acc > results[\"acc\"] or full_reward > results[\"reward\"]:\n",
    "                results[\"reward\"] = full_reward\n",
    "                results[\"episode\"] = episode\n",
    "                results[\"acc\"] = acc\n",
    "                results[\"features\"] = selected_features\n",
    "            for agent_idx in range(self.env.n_agents):\n",
    "                # 移除当前智能体所选特征的集合\n",
    "                without_agent_features = full_features - group_features[agent_idx]\n",
    "                # 计算包含和不包含当前智能体所选特征时的奖励\n",
    "                without_reward, _ = self.env.calculate_reward(list(without_agent_features),selected_batches, clf)\n",
    "                # 计算边缘奖励\n",
    "                marginal_reward = full_reward - without_reward\n",
    "                # # marginal_rewards.append(marginal_reward)\n",
    "                # with torch.no_grad():\n",
    "                #     q_values = self.agents[agent_idx][0](state)\n",
    "                #     next_q_values = self.agents[agent_idx][1](next_state)\n",
    "                #     td_error = torch.abs(marginal_reward + self.gamma * next_q_values.max() - q_values.max())\n",
    "                priorities = (marginal_reward + 1e-5)\n",
    "                transition = self._to_transition(state, marginal_reward, actions[agent_idx], next_state)  # 使用边缘特征奖励\n",
    "                self.agents[agent_idx][0].memory.add_real(transition, priorities)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if episode > 100 :\n",
    "                #一百次正常训练后，开始训练GAN\n",
    "                if Flag:\n",
    "                    \n",
    "                    self.train_GAN()\n",
    "                    Flag = False\n",
    "                    # 填满整棵树\n",
    "                    self.generate_and_add_fake_data(self.batch_size * 7)\n",
    "                else:\n",
    "                    self.generate_and_add_fake_data(self.batch_size)\n",
    "            if episode > self.batch_size:\n",
    "                self._update_models_tree(Flag, episode)\n",
    "            epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "            print(f\"Episode {episode:03d} | Reward: {full_reward:.8f} | Features: {len(selected_features)} | ACC: {acc:.8f}\")\n",
    "        return results\n",
    "\n",
    "    # 使用经验管理进行更新\n",
    "    def _update_models_tree(self, flag, episode):\n",
    "\n",
    "        should_update_target = episode % self.TARGET_REPLACE_ITER == 0\n",
    "\n",
    "        # 批量处理所有智能体的更新\n",
    "        for agent_idx, (online_agent, target_agent) in enumerate(self.agents):\n",
    "            if flag:\n",
    "                transitions = online_agent.memory.sample_true(self.batch_size)\n",
    "            else:\n",
    "                transitions = online_agent.memory.sample(self.batch_size)\n",
    "\n",
    "            # try:\n",
    "            # 提前将数据转换为列表，避免多次遍历 transitions\n",
    "            states = [x[0] for x in transitions]\n",
    "            actions = [x[1] for x in transitions]\n",
    "            rewards = [x[3] for x in transitions]\n",
    "            next_states = [x[2] for x in transitions]\n",
    "\n",
    "            # 一次性将数据转换为张量并移动到指定设备\n",
    "            s = torch.stack([torch.as_tensor(state, dtype=torch.float32) for state in states]).to(device)\n",
    "            a = torch.stack([torch.as_tensor(action, dtype=torch.int64) for action in actions]).to(device)\n",
    "            r = torch.stack([torch.as_tensor(reward, dtype=torch.float32) for reward in rewards]).to(device)\n",
    "            s_next = torch.stack([torch.as_tensor(next_state, dtype=torch.float32) for next_state in next_states]).to(device)\n",
    "        \n",
    "\n",
    "            # 计算当前Q值\n",
    "            q_values = online_agent(s)\n",
    "            if a.dim() == 1:\n",
    "                a = a.unsqueeze(-1)\n",
    "            q_eval = q_values.gather(1, a)\n",
    "\n",
    "            # 计算目标Q值，使用 no_grad 避免不必要的梯度计算\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_agent(s_next)\n",
    "                max_next_q = next_q_values.max(dim=1, keepdim=True)[0]\n",
    "                q_target = r.unsqueeze(-1) + self.gamma * max_next_q\n",
    "\n",
    "            # 计算损失\n",
    "            loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "            # 反向传播和参数更新\n",
    "            self.optimizers[agent_idx].zero_grad()  # 先清零梯度\n",
    "            loss.backward(retain_graph=True)\n",
    "            # 梯度裁剪，防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(online_agent.parameters(), max_norm=1.0)\n",
    "            self.optimizers[agent_idx].step()\n",
    "\n",
    "            # 更新目标网络\n",
    "            if should_update_target:\n",
    "                # 使用参数过滤加载\n",
    "                target_agent.load_state_dict(\n",
    "                    {k: v for k, v in online_agent.state_dict().items() \n",
    "                     if 'generator' not in k and 'discriminator' not in k},\n",
    "                    strict=False\n",
    "                )\n",
    "\n",
    "    # 转换保存格式        \n",
    "    def _to_transition(self, state, reward, action, next_state):\n",
    "        if isinstance(action, torch.Tensor):\n",
    "            action = action[0].item()\n",
    "        if isinstance(reward, torch.Tensor):\n",
    "            reward = reward[0].item()\n",
    "        return (state, action, next_state, reward)\n",
    "\n",
    "    # 训练GAN网络\n",
    "    def train_GAN(self):\n",
    "        for agent_idx, (eval_agent, target_agent) in enumerate(self.agents):\n",
    "            for _ in range(100):\n",
    "                opt_gen = optim.Adam(self.generators[agent_idx].parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "                opt_dis = optim.Adam(self.discriminators[agent_idx].parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "                z = torch.randn(self.batch_size, 32).to(device)\n",
    "                \n",
    "                transitions = eval_agent.memory.sample_true(self.batch_size)\n",
    "                # 添加数据校验\n",
    "                valid_transitions = [\n",
    "                    x for x in transitions \n",
    "                    if isinstance(x, (tuple, list)) and len(x) >=4\n",
    "                ]\n",
    "                if len(valid_transitions) == 0:\n",
    "                    print(\"警告：缓冲区无有效数据，跳过GAN训练\")\n",
    "                    return\n",
    "                    \n",
    "                try:\n",
    "                    # 使用更安全的转换方式\n",
    "                    s_rea = torch.stack([torch.as_tensor(x[0], dtype=torch.float32).to(device) for x in valid_transitions])\n",
    "                    a_rea = torch.stack([torch.as_tensor(x[1], dtype=torch.int).to(device) for x in valid_transitions])\n",
    "                    r_rea = torch.stack([torch.as_tensor(x[3], dtype=torch.float32).to(device) for x in valid_transitions])\n",
    "                    s_next_rea = torch.stack([torch.as_tensor(x[2], dtype=torch.float32).to(device) for x in valid_transitions])\n",
    "                except Exception as e:\n",
    "                    print(f\"数据转换异常: {e}\")\n",
    "                    print(\"问题数据样本:\", [x[:2] for x in transitions[:3]])  # 打印前3个样本的前两个元素\n",
    "                    return\n",
    "\n",
    "                split_sizes = [\n",
    "                    self.env.gat.output_dim,\n",
    "                    1, \n",
    "                    1, \n",
    "                    self.env.gat.output_dim\n",
    "                ]\n",
    "\n",
    "                # 生成假数据\n",
    "                fake_data = self.generators[agent_idx](z).detach() \n",
    "                s_gen, a_gen, r_gen, s_next_gen = fake_data.split(split_sizes, dim=1)\n",
    "                \n",
    "                self.generators[agent_idx].eval()  # 固定生成器\n",
    "                self.discriminators[agent_idx].train()  # 启用判别器梯度\n",
    "                \n",
    "                real_validity = self.discriminators[agent_idx](s_rea, a_rea, r_rea, s_next_rea)\n",
    "                fake_validity = self.discriminators[agent_idx](s_gen.detach(), a_gen.detach(), r_gen.detach(), s_next_gen.detach())\n",
    "                \n",
    "                loss_dis = -torch.mean(real_validity) + torch.mean(fake_validity)\n",
    "                \n",
    "                opt_dis.zero_grad()\n",
    "                loss_dis.backward(retain_graph=True)  # 保留计算图\n",
    "                torch.nn.utils.clip_grad_norm_(self.discriminators[agent_idx].parameters(), 1.0)  # 梯度裁剪\n",
    "                opt_dis.step()\n",
    "                \n",
    "                self.generators[agent_idx].train()  # 启用生成器梯度\n",
    "                self.discriminators[agent_idx].eval()  # 固定判别器\n",
    "                \n",
    "                # 重新生成数据（保持梯度）\n",
    "                fake_data_new = self.generators[agent_idx](z)\n",
    "                s_gen_new, a_gen_new, r_gen_new, s_next_gen_new = fake_data_new.split(split_sizes, dim=1)\n",
    "                \n",
    "                fake_validity_new = self.discriminators[agent_idx](s_gen_new, a_gen_new, r_gen_new, s_next_gen_new)\n",
    "                loss_gen = -torch.mean(fake_validity_new)\n",
    "                \n",
    "                opt_gen.zero_grad()\n",
    "                loss_gen.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.generators[agent_idx].parameters(), 1.0)  # 梯度裁剪\n",
    "                opt_gen.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0850ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"特征级自动编码器\"\"\"\n",
    "    def __init__(self, num_features, num_samples):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_samples, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_samples)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return z, self.decoder(z)\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"图注意力网络\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, heads=4):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads)\n",
    "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1)\n",
    "        self.output_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = nn.functional.elu(self.gat1(x, edge_index))\n",
    "        return self.gat2(x, edge_index)\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"图卷积网络\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.output_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return self.gcn2(x, edge_index)\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    \"\"\"决策智能体\"\"\"\n",
    "    def __init__(self, state_dim, target=True, output_dim=2):\n",
    "        super().__init__()\n",
    "        if target:\n",
    "            self.target = False\n",
    "            self.memory = PriorityReplayBuffer(real_capacity=100,gen_capacity=100)\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            self.fc1,\n",
    "            self.relu,\n",
    "            self.fc2\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.output_dim)\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self(state)).item()\n",
    "\n",
    "    \n",
    "# GAN模块\n",
    "# 生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, noise_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, state_dim * 2 + action_dim * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "# 判别器\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim * 2 + action_dim * 2, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, s, a, r, s_next):\n",
    "        a = a.unsqueeze(1) if a.dim() == 1 else a\n",
    "        r = r.unsqueeze(1) if r.dim() == 1 else r\n",
    "        x = torch.cat([s, a, r, s_next], dim=1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c9c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 5\n",
    "for file_name in config.DATASET_NAMES:\n",
    "\n",
    "    X_train, y_train, X_test, y_test, size = get_data(file_name)\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_train = X_train.values\n",
    "    if isinstance(X_test, pd.DataFrame):\n",
    "        X_test = X_test.values\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train = y_train.values\n",
    "    if isinstance(y_test, pd.Series):\n",
    "        y_test = y_test.values\n",
    "    # y_train = y_train.values\n",
    "    # X_test = X_test.values\n",
    "    # y_test = y_test.values\n",
    "\n",
    "    if X_train.shape[1]>4000:\n",
    "        continue\n",
    "    \n",
    "    if X_train.shape[0] >3000000:\n",
    "        device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(\"cuda:1\")\n",
    "    print(\"==================数据读取结束==================\")\n",
    "    # print(\"====================数据处理====================\")\n",
    "    # mi_matrix = compute_mi(X_train, X_train.shape[1])\n",
    "    # print(f\"MI矩阵维度: {mi_matrix.shape} (应等于特征数平方)\")\n",
    "    # assert mi_matrix.shape == (X_train.shape[1], X_train.shape[1]), \"MI矩阵维度错误\"\n",
    "    # laplacian = np.diag(np.sum(mi_matrix, axis=1)) - mi_matrix\n",
    "    # eigvals = np.sort(np.linalg.eigvalsh(laplacian))\n",
    "    \n",
    "    # # 寻找特征值拐点（经验法则）\n",
    "    # eig_diff = np.diff(eigvals[:20])\n",
    "    # n_groups = np.argmax(eig_diff) + 1  # 加1因为diff后索引偏移\n",
    "    # # n_groups_num = X_train.shape[1] // 5 + 1\n",
    "    # if n_groups < X_train.shape[1] // 10 + 1:\n",
    "    #     n_groups = X_train.shape[1] // 5 + 1\n",
    "    # print(f\"特征分组数量: {n_groups}\")\n",
    "    # feature_groups = feature_clustering_mi(mi_matrix, n_groups)\n",
    "    # print(f\"特征分组数量: {len(feature_groups)}\")\n",
    "    # print(feature_groups)\n",
    "    # all_features = set([f for group in feature_groups for f in group])\n",
    "    # assert len(all_features) == X_train.shape[1], \"特征分组存在遗漏\"\n",
    "    # assert max(all_features) < X_train.shape[1], \"存在非法特征索引\"\n",
    "    # print(\"==================数据处理结束==================\")\n",
    "    # 初始化环境\n",
    "    env = FeatureSelectionEnv(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"====================开始训练====================\")\n",
    "    for clf_name, clf in config.CLASSFIERS.items():\n",
    "        env.cache = {}\n",
    "        print(f\"====================开始训练：{clf_name}====================\")\n",
    "        # 训练系统\n",
    "        mas = MultiAgentSystem(env)\n",
    "        start = timeit.default_timer()\n",
    "        result = mas.train_with_gan(clf, dqn_episodes=300)\n",
    "        end = timeit.default_timer()\n",
    "        print(\"====================处理结束====================\")\n",
    "        print(\"====================最终评估====================\")\n",
    "\n",
    "        # 最终评估\n",
    "        selected_features = result[\"features\"]\n",
    "        clf.fit(X_train[:, selected_features], y_train)\n",
    "        y_pred = clf.predict(X_test[:, selected_features])\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        print(f\"\\n最终结果: 选择{len(selected_features)}个特征, 测试准确率: {acc:.4f}, F1分数: {f1:.4f}, 召回率: {recall:.4f}\")\n",
    "\n",
    "        # 保存结果\n",
    "        save_result_csv(config.RESULT_FILE,'MGFSBG',clf_name, size,file_name, acc, f1, recall, selected_features,time=end-start)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
